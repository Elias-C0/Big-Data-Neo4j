{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3efe19f",
   "metadata": {},
   "source": [
    "Descargarmos el archivo que vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800424de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"andrewmvd/spotify-playlists\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a0e0d",
   "metadata": {},
   "source": [
    "Limpiamos este archivo para que pueda ser leido por Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_spotify_dataset(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Limpia el dataset de Spotify eliminando filas con número incorrecto de columnas\n",
    "    y asegurando que no haya problemas de formato.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Iniciando limpieza: {start_time}\")\n",
    "\n",
    "    # Estadísticas\n",
    "    total_rows = 0\n",
    "    processed_rows = 0\n",
    "    skipped_rows = 0\n",
    "    column_distribution = {}\n",
    "\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8', errors='ignore') as infile, \\\n",
    "             open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "\n",
    "            # Leer primera línea para obtener encabezados\n",
    "            header_line = next(infile)\n",
    "            expected_columns = header_line.count(',') + 1\n",
    "            print(f\"Encabezados detectados: {expected_columns} columnas\")\n",
    "\n",
    "            # Configurar writer\n",
    "            writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "            # Escribir encabezados\n",
    "            writer.writerow([\"user_id\", \"artistname\", \"trackname\", \"playlistname\"])\n",
    "\n",
    "            # Procesar línea por línea para mayor control\n",
    "            line_num = 1  # Empezamos en 1 porque ya leímos la primera línea\n",
    "            for line in infile:\n",
    "                line_num += 1\n",
    "                total_rows += 1\n",
    "\n",
    "                # Contar columnas (método básico)\n",
    "                num_columns = line.count(',') + 1\n",
    "\n",
    "                # Actualizar estadísticas de distribución de columnas\n",
    "                if num_columns in column_distribution:\n",
    "                    column_distribution[num_columns] += 1\n",
    "                else:\n",
    "                    column_distribution[num_columns] = 1\n",
    "\n",
    "                # Saltarse líneas con número incorrecto de columnas\n",
    "                if num_columns != expected_columns:\n",
    "                    skipped_rows += 1\n",
    "                    if skipped_rows <= 5:  # Mostrar solo las primeras 5 líneas saltadas\n",
    "                        print(f\"Saltando línea {line_num}: {num_columns} columnas encontradas\")\n",
    "                    continue\n",
    "\n",
    "                # Procesar la línea manualmente para asegurar formato correcto\n",
    "                try:\n",
    "                    # Intentar analizar la línea con CSV reader\n",
    "                    fields = list(csv.reader([line]))[0]\n",
    "\n",
    "                    # Verificar número de campos después del parseo\n",
    "                    if len(fields) != expected_columns:\n",
    "                        skipped_rows += 1\n",
    "                        continue\n",
    "\n",
    "                    # Limpiar cada campo\n",
    "                    clean_fields = []\n",
    "                    for field in fields:\n",
    "                        # Eliminar comillas adicionales, saltos de línea y espacios extras\n",
    "                        clean_field = field.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "                        clean_fields.append(clean_field)\n",
    "\n",
    "                    # Escribir fila limpia\n",
    "                    writer.writerow(clean_fields)\n",
    "                    processed_rows += 1\n",
    "\n",
    "                    # Mostrar progreso\n",
    "                    if processed_rows % 100000 == 0:\n",
    "                        elapsed = datetime.now() - start_time\n",
    "                        print(f\"Procesadas {processed_rows} filas ({skipped_rows} descartadas). Tiempo: {elapsed}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    skipped_rows += 1\n",
    "                    if skipped_rows <= 5:\n",
    "                        print(f\"Error procesando línea {line_num}: {str(e)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error general: {str(e)}\")\n",
    "\n",
    "    # Resumen final\n",
    "    end_time = datetime.now()\n",
    "    elapsed = end_time - start_time\n",
    "\n",
    "    print(\"\\n===== RESUMEN DE LIMPIEZA =====\")\n",
    "    print(f\"Tiempo total: {elapsed}\")\n",
    "    print(f\"Filas totales procesadas: {total_rows}\")\n",
    "    print(f\"Filas conservadas: {processed_rows} ({processed_rows/total_rows*100:.2f}%)\")\n",
    "    print(f\"Filas descartadas: {skipped_rows} ({skipped_rows/total_rows*100:.2f}%)\")\n",
    "\n",
    "    print(\"\\nDistribución de columnas:\")\n",
    "    for cols, count in sorted(column_distribution.items()):\n",
    "        print(f\"  {cols} columnas: {count} filas ({count/total_rows*100:.2f}%)\")\n",
    "\n",
    "    print(f\"\\nArchivo limpio guardado en: {output_file}\")\n",
    "    print(f\"Tamaño original: {os.path.getsize(input_file) / (1024*1024):.2f} MB\")\n",
    "    print(f\"Tamaño limpio: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Ejecutar limpieza\n",
    "input_file = \"spotify_dataset.csv\"  # Ajusta la ruta según corresponda\n",
    "output_file = \"spotify_clean.csv\"\n",
    "clean_spotify_dataset(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcb0b5",
   "metadata": {},
   "source": [
    "Una vez hecho esto tenemos que ir a las configuraciones de Neo4j para cambiar estos parametros: \n",
    "\n",
    "dbms.memory.heap.initial_size=2G\n",
    "\n",
    "dbms.memory.heap.max_size=3G\n",
    "\n",
    "dbms.memory.transaction.total.max=3G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a59454",
   "metadata": {},
   "source": [
    "Despues se ejecuta este codigo en Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46055321",
   "metadata": {},
   "source": [
    "// PASO 1: Crear índices primero (MUY IMPORTANTE para rendimiento)\n",
    "CREATE INDEX user_id_index IF NOT EXISTS FOR (u:User) ON (u.id);\n",
    "CREATE INDEX artist_name_index IF NOT EXISTS FOR (a:Artist) ON (a.name);\n",
    "CREATE INDEX track_name_index IF NOT EXISTS FOR (t:Track) ON (t.name);\n",
    "CREATE INDEX playlist_name_index IF NOT EXISTS FOR (p:Playlist) ON (p.name);\n",
    "\n",
    "// PASO 2: Cargar datos de muestra\n",
    "LOAD CSV WITH HEADERS FROM 'file:///spotify_clean.csv' AS row\n",
    "lIMIT 735000\n",
    "WITH row\n",
    "WHERE row.user_id IS NOT NULL \n",
    "  AND row.artistname IS NOT NULL \n",
    "  AND row.trackname IS NOT NULL \n",
    "  AND row.playlistname IS NOT NULL \n",
    "WITH\n",
    "  trim(row.user_id) AS user_id, \n",
    "  trim(row.artistname) AS artistname, \n",
    "  trim(row.trackname) AS trackname, \n",
    "  trim(row.playlistname) AS playlistname \n",
    "\n",
    "// Crear usuario \n",
    "MERGE (u:User {id: user_id}) \n",
    "\n",
    "// Crear artista \n",
    "MERGE (a:Artist {name: artistname}) \n",
    "\n",
    "// Crear track y relación con artista \n",
    "MERGE (t:Track {name: trackname}) \n",
    "MERGE (t)-[:BY]->(a) \n",
    "\n",
    "// Crear playlist \n",
    "MERGE (p:Playlist {name: playlistname}) \n",
    "MERGE (u)-[:CREATED]->(p) \n",
    "MERGE (p)-[:CONTAINS]->(t); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafcf98",
   "metadata": {},
   "source": [
    "Algunas consultas:\n",
    "\n",
    "// Top 15 playlist con mas canciones\n",
    "MATCH (p:Playlist)-[:CONTAINS]->(t:Track)\n",
    "RETURN p.name AS Playlist, count(t) AS NumCanciones\n",
    "ORDER BY NumCanciones DESC\n",
    "LIMIT 15;\n",
    "\n",
    "// Top 15 artistas más populares (con más canciones en playlists)\n",
    "MATCH (a:Artist)<-[:BY]-(t:Track)<-[:CONTAINS]-(p:Playlist)\n",
    "RETURN a.name AS Artista, count(DISTINCT t) AS NumCanciones, count(DISTINCT p) AS NumPlaylists\n",
    "ORDER BY NumPlaylists DESC\n",
    "LIMIT 15;\n",
    "\n",
    "//Mostrar 800 nodos\n",
    "MATCH (n) RETURN n LIMIT 800;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7ed43",
   "metadata": {},
   "source": [
    "En caso de querer ver los 800 nodos en pantalla debe cambiar el parametro de visualizacion de cypher:\n",
    "Graph Visualization\n",
    "Initial Node Display 1000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
